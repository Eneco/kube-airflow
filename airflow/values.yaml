# Duplicate this file and put your customization here

##
## common settings and setting for the webserver
airflow:
  ##
  ## You will need to define your frenet key:
  ## Generate fernet_key with:
  ##    python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)"
  ## fernet_key: ABCDABCDABCDABCDABCDABCDABCDABCDABCDABCD
  fernet_key: ""
  service:
    type: ClusterIP
  ##
  ## mount path for persistant volume
  dag_path: /dags
  ##
  ## set the max number of retries during container initialization
  init_retry_loop:
  ##
  ## base image for webserver/scheduler/workers
  ## Note: If you want to use airflow HEAD (2.0dev), use the following image:
  # image
  #   repository: stibbons31/docker-airflow-dev
  #   tag: 2.0dev
  image:
    ##
    ## docker-airflow image
    repository: puckel/docker-airflow
    ##
    ## image tag
    tag: 1.9.0
    ##
    ## Image pull policy
    ## values: Always or IfNotPresent
    pull_policy: IfNotPresent
  ##
  ## Set scheduler_num_runs to control how the schduler behaves:
  ##   -1 will let him looping indefinitively but it will never update the DAG
  ##   1 will have the scheduler quit after each refresh, but kubernetes will restart it
  scheduler_num_runs: "-1"
  ##
  ## how many replicas for web server
  web_replicas: 2
  ##
  ## Custom airflow configuration environment variables
  config: {}

##
## Configuration for celery workers
celery:
  ##
  ## Number of celery workers to initialize
  num_workers: 1

##
## Ingress configuration
ingress:
  ##
  ## enable ingress
  ## Note: If you want to change url prefix for web ui or flower even if you do not use ingress,
  ## you can still change ingress.path.web and ingress.path.flower
  enabled: false
  ##
  ## Define the annotation here to configure rewriting rules related to your Load balancer
  #
  ## Please note their is a small difference between the way Airflow Web server and Flower handles
  ## URL prefixes in HTTP requests:
  ##  - airflow webserver handles it completely, just let your load balancer to give the HTTP
  ##    header like the requested URL (no special configuration neeed)
  ##  - Flower wants HTTP header to behave like there was no URL prefix, and but still generates
  ##    the right URL in html pages.
  #
  ##    Extracted from the Flower documentation:
  ##    (https://github.com/mher/flower/blob/master/docs/config.rst#url_prefix)
  #
  ##        To access Flower on http://example.com/flower run it with:
  ##            flower --url_prefix=flower
  #
  ##        Use the following nginx configuration:
  ##            server {
  ##              listen 80;
  ##              server_name example.com;
  #
  ##              location /flower/ {
  ##                rewrite ^/flower/(.*)$ /$1 break;
  ##                proxy_pass http://example.com:5555;
  ##                proxy_set_header Host $host;
  ##              }
  ##            }
  annotations:
    ## Annotations for the webserver
    web:
      ##
      ## Example for Traefik:
      # traefik.frontend.rule.type: PathPrefix
      # kubernetes.io/ingress.class: traefik
    ##
    ## Annotations for the webserver
    flower:
      ##
      ## Example for Traefik:
      # traefik.frontend.rule.type: PathPrefixStrip
      # kubernetes.io/ingress.class: traefik
  ##
  ## hostname of the webserver and the flower
  host: ""
  ## Configure path mount in URLs
  path:
    ##
    ## if web is '/airflow':
    ##  - UI will be accessible to '/airflow/admin'
    ##  - Healthcheck is at 'http://mycompany.com/airflow/health'
    ##  - api is at 'http://mycompany.com/airflow/api'
    web: /
    ##
    ## if flower is '/airflow/flower':
    ##  - api is at 'http://mycompany.com/airflow/flower'
    flower: /flower

##
## Storage configuration
persistence:
  ##
  ## enable persistance storage
  enabled: false
  ##
  ## Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  # storageClass: default
  accessMode: ReadWriteOnce
  ##
  ## Persistant storage size request
  size: 1Gi

##
## Configure DAGs deployment and update
dags:
  ##
  ## pickle_dag: send DAG using pickle from the scheduler to the worker
  pickle_dag: true
  ##
  ## Force a requirements.txt file to be present at the root of the image so these Python packages
  ## will be installed
  ## Should be a multiline text respecting the requirements.txt format (pip compatible)
  requirements: |
    yaml
  ##
  ## Use of git-sync: beware when using git-sync and airflow. If the scheduler reloads a dag in the
  ## middle of a dagrun then the dagrun will actually start using the new version of the dag in the
  ## middle of execution.
  ## This is a known issue with airflow and it means it's unsafe in general to use a git-sync
  ## like solution with airflow without:
  ## - using explicit locking, ie never pull down a new dag if a dagrun is in progress
  ## - make dags immutable, never modify your dag always make a new one
  git_sync_enabled: false
  ##
  ## url to clone the git repository
  git_repo:
  ##
  ## branch name to clone
  git_branch: master
  ##
  ## poll interval in seconds between two refreshes
  poll_interval_sec: 60
  ##
  ## print debug logs
  git_sync_debug: false
  ##
  ## Disable Load examples as soon as you enable git_repo
  load_examples: true

## Configuration values for the postgresql dependency.
## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md
##
postgresql:
  ##
  ## Use the PostgreSQL chart dependency.
  ## Set to false if bringing your own PostgreSQL.
  enabled: true
  ##
  ## If bringing your own PostgreSQL, the full uri to use
  ## e.g. postgres://airflow:changeme@my-postgres.com:5432/airflow?sslmode=disable
  # uri:
  ##
  ### PostgreSQL User to create.
  postgresUser: airflow
  ##
  ## PostgreSQL Password for the new user.
  ## If not set, a random 10 characters password will be used.
  postgresPassword: airflow
  ##
  ## PostgreSQL Database to create.
  postgresDatabase: airflow
  ##
  ## Persistent Volume Storage configuration.
  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes
  persistence:
    ##
    ## Enable PostgreSQL persistence using Persistent Volume Claims.
    enabled: true

## Configuration values for the postgresql dependency.
## ref: https://github.com/kubernetes/charts/blob/master/stable/redis/README.md
redis:
  ##
  ## Use the redis chart dependency.
  ## Set to false if bringing your own redis.
  enabled: true
  ##
  ## Redis password
  redisPassword: redis
  persistence:
    ##
    ## Use a PVC to persist data
    enabled: true
